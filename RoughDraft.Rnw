\documentclass{article}
\usepackage{float, hyperref}
\usepackage[margin=1in]{geometry}

\begin{document}

\author{Danny Bero}
\title{STAT 585 Project: Are newer songs becoming more homogenous?}
\maketitle


\section*{Introduction}

It seems that everytime I hear the newest hit song on the radio, I am hearing a modified version of the last hit from a couple of weeks ago. The hit, of course, will begin with a verse, then a chorus, then another verse (which sounds strangely similar to the previous verse), and then another chorus. This will repeat until, if the artist is feeling bold, the song will have its "breakdown" and then end with a couple of chorus' in a row. It seems a blueprint has been made and every new pop artist follows it verbatim. This seems rather different from the chart-topping songs from the past, which seemed to follow no blueprint and represented many genres of music. This leads me to the main question of my analysis, are newer "hit" songs becoming more homogenous? And if so, can we prove it using statistics? In order to answer this question I will be randomly choosing ten songs from 1965 to 1975 and ten songs from 2004 to 2014 and checking if the differences between the songs have gotten smaller over time. First, we will take a look at the R package that will be used for the sound files, tuneR. Next, we will devise a reproducible plan to randomly select these 20 songs. After that, a function must be written to create a data table of summary statstics for the songs. Finally, we will conduct the necessary analysis to answer the question of interest. 

\section{Introduction to the tuneR Package}

Enter the tuneR package. Using tuneR, we can load in mp3 or wave files using commands such as readMP3() or readWave(). The song is converted into sort of a time series plot of frequency over time. We can see an example of how to read in data and what the data looks like in the plots below. Note that "jude.wav" is a 45 second clip of the song "Hey Jude" by the Beatles stored in a Wave file. 

<<libraries.1, echo = FALSE>>=
library(xtable)
@

<<libraries.2,message=FALSE>>=
library(tuneR)
@

<<tuneRplot, fig.width=4, fig.height=4, out.width='\\textwidth'>>=
HeyJude = readWave("jude.wav")
plot(HeyJude, lwd = 1/2, cex.lab = 0.1)
@

HeyJude is now an S4 object and we can access and manipulate each channel using HeyJude@left or HeyJude@right. HeyJude@right is a numeric vector of amplitudes at each point in time and we can calculate summary statistics such as the mean or median using simple R commands like so.

<<tuneRmean, fig.width=4, fig.height=3, out.width='\\textwidth'>>=
mean(HeyJude@right)
@

\section{songtoDF() Function}

Using these summary statistics and the tuneR packages, we can create a function called `songtoDF()` that takes in a wave file and outputs a data frame. The summary statistics of interest are the mean, standard deviation, median, min, max, and range of both the left and right channels. Note that a song with only one channel stores that channel in @left. Therefore, in order to get data for the right channel we will set the @right equal to @left in this case. The additional function `songstoDF()` applies the function `songtoDF()` to a vector of wave files and a vector of song names. 

<<songtoDF>>=
songtoDF = function(song){
  x = noSilence(song)
  left = x@left
  right = x@right
  d.left = data.frame(mean.l = mean(left), sd.l = sd(left), median.l = median(left), min.l = min(left), 
                 max.l = max(left), range.l = max(left) - min(left))
  if(mean(right) == "NaN"){
    d.right = data.frame(mean.r = mean(left), sd.r = sd(left), median.r = median(left), min.r = min(left),
                         max.r = max(left), range.r = max(left) - min(left))
  } else{
    d.right = data.frame(mean.r = mean(right), sd.r = sd(right), median.r = median(right), min.r = min(right), 
                 max.r = max(right), range.r = max(right) - min(right))
  }
  d = data.frame(d.left, d.right)
  return(d)
}


songstoDF = function(songs, names){
  data = NULL
  for(i in 1:length(songs)){
    d = songtoDF(songs[[i]])
    if(i == 1){data = d} else {data = rbind(data, d)}
  }
  end = cbind(names,data)
  return(end)
}
@

\section{Selecting the Songs}

Now that a function has been created to convert a list of songs to a data frame, we will have to devise a plan to pick the songs to be analyzed. This could be done in many ways, but the following is the solution I came up with. First, we will only consider the subset of hit songs that made number one on the Billboard Hot 100. We will also only use songs from 1965 to 1975, "oldies," and songs from 2004 to 2014, "new songs." The following function has been written to scrape all number one songs from a specific year off of the website www.billboard.com. 

<<getyear>>=
getyear = function(year) {
  url = sprintf("http://www.billboard.com/archive/charts/%d/hot-100", year)
  table = readHTMLTable(url)$`NULL`
  
  # rename the columns
  names(table)[1] = "Week"
  names(table)[2] = "Song"
  names(table)[3] = "Artist"
  
  ## This function makes sure there is a song and artist for each week. Repeats were originally
  ## left as <NA>
  for(i in 1:nrow(table)){
    if(is.na(table[i,]$Song)){
      table[i,]$Song = table[i-1,]$Song
    } else{
      table[i,]$Song = table[i,]$Song
    }
    if(is.na(table[i,]$Artist)){
      table[i,]$Artist = table[i-1,]$Artist
    } else{
      table[i,]$Artist = table[i,]$Artist
    }
  }
  return(table)
}
@

The function that is used to scrape all songs from 1958 to 2014 is left out because it takes quite a bit of time to run. Below is the function that randomly selected 10 songs from 1965 to 1975 and ten songs from 2004 to 2014. Note that it is not evaluated because the dataset with all of the songs in it is left out. 

<<data.s, echo = FALSE>>=
data.s = read.csv("data.s.csv")
@

<<decade.data, eval = FALSE>>=
data.6070s = data.s[which(data.s$Year %in% 1965:1975),]
set.seed(123456)
oldies = data.6070s[sample(1:nrow(data.6070s), 10, replace=F),]

data.0010s = data.s[which(data.s$Year %in% 2004:2014),]
set.seed(123456)
newsongs = data.0010s[sample(1:nrow(data.0010s), 10, replace=F),]
@

We have songs! Below is the data table with the list of songs selected. 

<<songdata, echo=FALSE>>=
songdata = read.csv("songdata.csv")
@

<<songdata.display, results="asis", echo=FALSE>>=
xtable(songdata[,-c(1:2)])
@

\section{Completing the Data Frame}

Now that we have the list of songs we will analyze, we can read the songs in and use `songstoDF()` to get a data frame. Note that the songs were clipped to 45 second chunks specificaly taken from 1:00 minutes to 1:45 minuutes in order to be uniform. This was done because the full length songs were too much for R to read in. Finally, we can column bind the data table from the previous part to the data table from `songstoDF()`. This will create a full data set that will be simple for analysis.

<<final.data, eval = FALSE>>=
songs = c(HalfBreed, Georgia, Aquarius, HeyJude, LoveChild, Drag, IllBeThere, WorkItOut,
          Fame, ReachOut, Stronger, Jagger, NoOne, Umbrella, BigGirls, DontForget, Boom,
          DropIt, SaySomething, WeBelong)

names = c("HalfBreed", "Georgia", "Aquarius", "HeyJude", "LoveChild", "Drag", "IllBeThere", 
          "WorkItOut", "Fame", "ReachOut", "Stronger", "Jagger", "NoOne", "Umbrella", 
          "BigGirls", "DontForget", "Boom", "DropIt", "SaySomething", "WeBelong")

info = songstoDF(songs, names)

data = cbind(songdata[,-1], info[,-1])
decade = c(rep("old",10), rep("new",10))
data = cbind(data, decade)
data = data[,-1]
@

<<read.data, echo=FALSE>>=
data = read.csv("data.csv")
@

<<data, echo = FALSE, results="asis">>=
xtable(data[,c(3,6:11)])
@

Above is an example of what the data table looks like. This is just the summary data for the left channel, the full dataset includes both left and right channels. 

\section{Analysis: Teaser}

When looking at some plots of the data, the evidence, however small, seems to point to the fact that songs \textit{are} becoming more homogenous. When looking at a plot of the standard deviation of the right channel versus the mean of the right channel, below, we can see new songs all have a mean \textit{very} close to zero, while oldies have means all over. It can also be noted that the standard deviation doesn't seem to have an effect on the mean. 

<<libraries.3, echo=FALSE>>=
library(ggplot2)
@

<<mean vs sd, message = F>>=
qplot(x = sd.r, y = mean.r, color = decade, data = data) + geom_smooth(aes(group = decade), se = F)
@

In addition to the previous plot, a plot of the min of the right channel versus the max of the right channel shows that new songs follow a linear trend. The min is some function of the max and visa versa. But, when looking at the oldies, there seems to be less of a linear trend and the data is a lot more variable. This adds to the evidence that newer songs follow some "recipe" while older songs were more variable. Note: The last two plots were for right channel only. The left channel looks very similar and therefore, was not included. 

<<min vs max, message = F>>=
qplot(max.r, min.r, color = decade, data = data) + geom_smooth(aes(group = decade), se = F)
@

\end{document}